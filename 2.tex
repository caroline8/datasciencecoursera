% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Untitled},
  pdfauthor={you},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Untitled}
\author{you}
\date{2020/7/24}

\begin{document}
\maketitle

\hypertarget{how-to-share-data-with-a-statistician}{%
\section{How to share data with a
statistician}\label{how-to-share-data-with-a-statistician}}

This is a guide for anyone who needs to share data with a statistician
or data scientist. The target audiences I have in mind are:

\begin{itemize}
\tightlist
\item
  Collaborators who need statisticians or data scientists to analyze
  data for them
\item
  Students or postdocs in various disciplines looking for consulting
  advice
\item
  Junior statistics students whose job it is to collate/clean/wrangle
  data sets
\end{itemize}

The goals of this guide are to provide some instruction on the best way
to share data to avoid the most common pitfalls and sources of delay in
the transition from data collection to data analysis. The
\href{http://biostat.jhsph.edu/~jleek/}{Leek group} works with a large
number of collaborators and the number one source of variation in the
speed to results is the status of the data when they arrive at the Leek
group. Based on my conversations with other statisticians this is true
nearly universally.

My strong feeling is that statisticians should be able to handle the
data in whatever state they arrive. It is important to see the raw data,
understand the steps in the processing pipeline, and be able to
incorporate hidden sources of variability in one's data analysis. On the
other hand, for many data types, the processing steps are well
documented and standardized. So the work of converting the data from raw
form to directly analyzable form can be performed before calling on a
statistician. This can dramatically speed the turnaround time, since the
statistician doesn't have to work through all the pre-processing steps
first.

\hypertarget{what-you-should-deliver-to-the-statistician}{%
\section{What you should deliver to the
statistician}\label{what-you-should-deliver-to-the-statistician}}

To facilitate the most efficient and timely analysis this is the
information you should pass to a statistician:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The raw data.
\item
  A \href{http://vita.had.co.nz/papers/tidy-data.pdf}{tidy data set}
\item
  A code book describing each variable and its values in the tidy data
  set.\\
\item
  An explicit and exact recipe you used to go from 1 -\textgreater{} 2,3
\end{enumerate}

Let's look at each part of the data package you will transfer.

\hypertarget{the-raw-data}{%
\subsubsection{The raw data}\label{the-raw-data}}

It is critical that you include the rawest form of the data that you
have access to. This ensures that data provenance can be maintained
throughout the workflow. Here are some examples of the raw form of data:

\begin{itemize}
\tightlist
\item
  The strange \href{http://en.wikipedia.org/wiki/Binary_file}{binary
  file} your measurement machine spits out
\item
  The unformatted Excel file with 10 worksheets the company you
  contracted with sent you
\item
  The complicated \href{http://en.wikipedia.org/wiki/JSON}{JSON} data
  you got from scraping the
  \href{https://twitter.com/twitterapi}{Twitter API}
\item
  The hand-entered numbers you collected looking through a microscope
\end{itemize}

You know the raw data are in the right format if you:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Ran no software on the data
\item
  Did not modify any of the data values
\item
  You did not remove any data from the data set
\item
  You did not summarize the data in any way
\end{enumerate}

If you made any modifications of the raw data it is not the raw form of
the data. Reporting modified data as raw data is a very common way to
slow down the analysis process, since the analyst will often have to do
a forensic study of your data to figure out why the raw data looks
weird. (Also imagine what would happen if new data arrived?)

\hypertarget{the-tidy-data-set}{%
\subsubsection{The tidy data set}\label{the-tidy-data-set}}

The general principles of tidy data are laid out by
\href{http://had.co.nz/}{Hadley Wickham} in
\href{http://vita.had.co.nz/papers/tidy-data.pdf}{this paper} and
\href{http://vimeo.com/33727555}{this video}. While both the paper and
the video describe tidy data using \href{http://www.r-project.org/}{R},
the principles are more generally applicable:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable you measure should be in one column
\item
  Each different observation of that variable should be in a different
  row
\item
  There should be one table for each ``kind'' of variable
\item
  If you have multiple tables, they should include a column in the table
  that allows them to be joined or merged
\end{enumerate}

While these are the hard and fast rules, there are a number of other
things that will make your data set much easier to handle. First is to
include a row at the top of each data table/spreadsheet that contains
full row names. So if you measured age at diagnosis for patients, you
would head that column with the name \texttt{AgeAtDiagnosis} instead of
something like \texttt{ADx} or another abbreviation that may be hard for
another person to understand.

Here is an example of how this would work from genomics. Suppose that
for 20 people you have collected gene expression measurements with
\href{http://en.wikipedia.org/wiki/RNA-Seq}{RNA-sequencing}. You have
also collected demographic and clinical information about the patients
including their age, treatment, and diagnosis. You would have one
table/spreadsheet that contains the clinical/demographic information. It
would have four columns (patient id, age, treatment, diagnosis) and 21
rows (a row with variable names, then one row for every patient). You
would also have one spreadsheet for the summarized genomic data. Usually
this type of data is summarized at the level of the number of counts per
exon. Suppose you have 100,000 exons, then you would have a
table/spreadsheet that had 21 rows (a row for gene names, and one row
for each patient) and 100,001 columns (one row for patient ids and one
row for each data type).

If you are sharing your data with the collaborator in Excel, the tidy
data should be in one Excel file per table. They should not have
multiple worksheets, no macros should be applied to the data, and no
columns/cells should be highlighted. Alternatively share the data in a
\href{http://en.wikipedia.org/wiki/Comma-separated_values}{CSV} or
\href{http://en.wikipedia.org/wiki/Tab-separated_values}{TAB-delimited}
text file. (Beware however that reading CSV files into Excel can
sometimes lead to non-reproducible handling of date and time variables.)

\hypertarget{the-code-book}{%
\subsubsection{The code book}\label{the-code-book}}

For almost any data set, the measurements you calculate will need to be
described in more detail than you can or should sneak into the
spreadsheet. The code book contains this information. At minimum it
should contain:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Information about the variables (including units!) in the data set not
  contained in the tidy data
\item
  Information about the summary choices you made
\item
  Information about the experimental study design you used
\end{enumerate}

In our genomics example, the analyst would want to know what the unit of
measurement for each clinical/demographic variable is (age in years,
treatment by name/dose, level of diagnosis and how heterogeneous). They
would also want to know how you picked the exons you used for
summarizing the genomic data (UCSC/Ensembl, etc.). They would also want
to know any other information about how you did the data
collection/study design. For example, are these the first 20 patients
that walked into the clinic? Are they 20 highly selected patients by
some characteristic like age? Are they randomized to treatments?

A common format for this document is a Word file. There should be a
section called ``Study design'' that has a thorough description of how
you collected the data. There is a section called ``Code book'' that
describes each variable and its units.

\hypertarget{how-to-code-variables}{%
\subsubsection{How to code variables}\label{how-to-code-variables}}

When you put variables into a spreadsheet there are several main
categories you will run into depending on their
\href{http://en.wikipedia.org/wiki/Statistical_data_type}{data type}:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Continuous
\item
  Ordinal
\item
  Categorical
\item
  Missing
\item
  Censored
\end{enumerate}

Continuous variables are anything measured on a quantitative scale that
could be any fractional number. An example would be something like
weight measured in kg.
\href{http://en.wikipedia.org/wiki/Ordinal_data}{Ordinal data} are data
that have a fixed, small (\textless{} 100) number of levels but are
ordered. This could be for example survey responses where the choices
are: poor, fair, good.
\href{http://en.wikipedia.org/wiki/Categorical_variable}{Categorical
data} are data where there are multiple categories, but they aren't
ordered. One example would be sex: male or female. This coding is
attractive because it is self-documenting.
\href{http://en.wikipedia.org/wiki/Missing_data}{Missing data} are data
that are unobserved and you don't know the mechanism. You should code
missing values as \texttt{NA}.
\href{http://en.wikipedia.org/wiki/Censoring_(statistics)}{Censored
data} are data where you know the missingness mechanism on some level.
Common examples are a measurement being below a detection limit or a
patient being lost to follow-up. They should also be coded as
\texttt{NA} when you don't have the data. But you should also add a new
column to your tidy data called, ``VariableNameCensored'' which should
have values of \texttt{TRUE} if censored and \texttt{FALSE} if not. In
the code book you should explain why those values are missing. It is
absolutely critical to report to the analyst if there is a reason you
know about that some of the data are missing. You should also not
\href{http://en.wikipedia.org/wiki/Imputation_(statistics)}{impute}/make
up/ throw away missing observations.

In general, try to avoid coding categorical or ordinal variables as
numbers. When you enter the value for sex in the tidy data, it should be
``male'' or ``female''. The ordinal values in the data set should be
``poor'', ``fair'', and ``good'' not 1, 2 ,3. This will avoid potential
mixups about which direction effects go and will help identify coding
errors.

Always encode every piece of information about your observations using
text. For example, if you are storing data in Excel and use a form of
colored text or cell background formatting to indicate information about
an observation (``red variable entries were observed in experiment 1.'')
then this information will not be exported (and will be lost!) when the
data is exported as raw text. Every piece of data should be encoded as
actual text that can be exported.

\hypertarget{the-instruction-listscript}{%
\subsubsection{The instruction
list/script}\label{the-instruction-listscript}}

You may have heard this before, but
\href{http://www.sciencemag.org/content/334/6060/1226}{reproducibility
is a big deal in computational science}. That means, when you submit
your paper, the reviewers and the rest of the world should be able to
exactly replicate the analyses from raw data all the way to final
results. If you are trying to be efficient, you will likely perform some
summarization/data analysis steps before the data can be considered
tidy.

The ideal thing for you to do when performing summarization is to create
a computer script (in \texttt{R}, \texttt{Python}, or something else)
that takes the raw data as input and produces the tidy data you are
sharing as output. You can try running your script a couple of times and
see if the code produces the same output.

In many cases, the person who collected the data has incentive to make
it tidy for a statistician to speed the process of collaboration. They
may not know how to code in a scripting language. In that case, what you
should provide the statistician is something called
\href{http://en.wikipedia.org/wiki/Pseudocode}{pseudocode}. It should
look something like:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Step 1 - take the raw file, run version 3.1.2 of summarize software
  with parameters a=1, b=2, c=3
\item
  Step 2 - run the software separately for each sample
\item
  Step 3 - take column three of outputfile.out for each sample and that
  is the corresponding row in the output data set
\end{enumerate}

You should also include information about which system
(Mac/Windows/Linux) you used the software on and whether you tried it
more than once to confirm it gave the same results. Ideally, you will
run this by a fellow student/labmate to confirm that they can obtain the
same output file you did.

\hypertarget{what-you-should-expect-from-the-analyst}{%
\section{What you should expect from the
analyst}\label{what-you-should-expect-from-the-analyst}}

When you turn over a properly tidied data set it dramatically decreases
the workload on the statistician. So hopefully they will get back to you
much sooner. But most careful statisticians will check your recipe, ask
questions about steps you performed, and try to confirm that they can
obtain the same tidy data that you did with, at minimum, spot checks.

You should then expect from the statistician:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  An analysis script that performs each of the analyses (not just
  instructions)
\item
  The exact computer code they used to run the analysis
\item
  All output files/figures they generated.
\end{enumerate}

This is the information you will use in the supplement to establish
reproducibility and precision of your results. Each of the steps in the
analysis should be clearly explained and you should ask questions when
you don't understand what the analyst did. It is the responsibility of
both the statistician and the scientist to understand the statistical
analysis. You may not be able to perform the exact analyses without the
statistician's code, but you should be able to explain why the
statistician performed each step to a labmate/your principal
investigator.

\hypertarget{contributors}{%
\section{Contributors}\label{contributors}}

\begin{itemize}
\tightlist
\item
  \href{http://biostat.jhsph.edu/~jleek/}{Jeff Leek} - Wrote the initial
  version.
\item
  \href{http://bit.ly/LColladoTorres}{L. Collado-Torres} - Fixed typos,
  added links.
\item
  \href{http://people.umass.edu/nick/}{Nick Reich} - Added tips on
  storing data as text.
\item
  \href{https://www.amherst.edu/people/facstaff/nhorton}{Nick Horton} -
  Minor wording suggestions.
\end{itemize}

\includegraphics{2_files/figure-latex/pressure-1.pdf}

\end{document}
